---
layout: single
title: "Time Travel with Delta-RS: Local Experiments with MinIO, Azurite, and Databricks"
excerpt: "Exploring Delta Lake‚Äôs Time Travel feature across environments ‚Äî MinIO over HTTPS, Azurite over HTTPS, and Databricks SQL ‚Äî using Python‚Äôs delta-rs library for local reproducibility."
date: 2025-10-19
classes:
  - center-page
author: michael_lockwood
author_profile: true
sidebar: false
toc: true
toc_label: "SECTIONS"
toc_icon: "list"
categories: [Data Engineering, Delta Lake, MinIO, Azurite, Databricks]
tags: [Delta-RS, MinIO, Azurite, Databricks, Time Travel, Parquet, Delta Lake]
header:
  overlay_color: "#000"
  overlay_filter: "0.75"
  overlay_image: /assets/images/default-overlay-wide.jpg 
  caption: "Local Time Travel experiments using Delta-RS with MinIO, Azurite, and Databricks SQL."
---


<a id="toc" class="visually-hidden"></a>

# Time Travel with Delta-RS: Local Experiments with MinIO, Azurite, and Databricks

## Introduction
- Brief context on Delta Lake and why ‚ÄúTime Travel‚Äù matters.  
- Mention my focus on local experimentation using **MinIO (HTTPS)** and **Azurite (HTTPS)** ‚Äî avoiding reliance on cloud-hosted services.  
- Note that this work complements my earlier projects on Parquet ingestion, Delta-RS integration, and Azure emulation.

## üïì The Concept of Time Travel
In Delta Lake, Time Travel means you can read a table as it existed at any earlier point, without restoring backups or duplicating datasets. It‚Äôs enabled by Delta‚Äôs ACID transaction log (the _delta_log folder next to my Parquet files), which records every commit as an immutable, ordered sequence.  

__Versioned Parquet snapshots__  
Each commit produces a new table version (v0, v1, v2, ‚Ä¶). A version is just a logical snapshot composed of the Parquet files that were added or removed in that commit. Delta uses small JSON (and periodic checkpoint) files to describe which data files are active for each version.  

__ACID guarantees__  
The transaction log enforces atomicity, consistency, isolation, and durability. Readers see a consistent snapshot (no partial writes), writers don‚Äôt corrupt the table if they fail midway, and concurrent operations serialize into clean versions.  

__Schema enforcement & evolution__  
The log also captures schema information. Delta can enforce the expected schema and, when configured, evolve it (e.g., add a column) in a controlled, versioned way. Time Travel lets you query before or after such changes.  

__Reproducibility__  
Because every version is addressable (by version number or timestamp), you can rerun analyses exactly as they were, audit historical states, compare results across points in time, and debug upstream changes without rolling back data.  

__Retention-aware__  
Time Travel relies on the presence of historical log entries and data files. If you VACUUM aggressively or shorten log retention, very old versions may no longer be queryable. In other words: retention policies define how far back you can travel.  

__Net effect__  
Delta‚Äôs Time Travel turns my Parquet dataset into a transactional, versioned table where historical reads are first-class‚Äîfast, reliable, and independent of backup/restore workflows.  
{: .fancy}

Contrast with traditional database recovery or restore scenarios in SQL Server.  
{: .md-h2i}


üëÄ Highlight its practical uses: debugging, auditability, reproducible analytics, and schema evolution.

## Setting the Stage
- Describe my lab environment:  
  - MinIO bucket (`nyctaxi-pipeline`) over HTTPS  
  - Azurite over HTTP for Azure Blob emulation  
  - Databricks SQL (DBX) for validation of versioned reads  
- Mention the dataset (NYC Taxi, reduced Parquet samples) and Delta folder structure (`data_in`, `data_out`, `delta`, `snapshots`).

> ![For Blog: MinIO Object Browser](/assets/images/minio-nyctaxi-pipeline.png)
> *MinIO Object Browser showing the `nyctaxi-pipeline` bucket structure.*

## Delta-RS and Local Connectivity
- Overview of Delta-RS (Rust implementation of Delta Lake) and why it‚Äôs ideal for lightweight, cross-platform experimentation.  
- Discuss connection differences:
  - `https://127.0.0.1:10000/...` for Azurite  
  - `https://127.0.0.1:9000/...` for MinIO  
- Mention challenges with Azurite HTTPS (URL rewriting, emulator redirection) and why MinIO proved stable.

## üî¨ Performing Time Travel
- Step-by-step summary of what the Time Travel operation does:
  1. Read a Delta table at its latest version.  
  2. Retrieve table history to list all commits.  
  3. Query a prior version using `version=n` or a timestamp.  
- Example outputs or command patterns (placeholder text for when you import screenshots/code).  
- Highlight that Delta-RS enforces version consistency even without Spark or JVM dependencies.

## Comparing Environments
- Observations comparing behavior between:
  - Delta-RS over Azurite (HTTP)  
  - Delta-RS over MinIO (HTTPS)  
  - Databricks SQL dashboard  
- Discuss any performance or compatibility nuances.  
- Mention how Databricks confirms Delta-RS read consistency.

## Lessons Learned
- Reflection on local-first development and emulation of cloud features.  
- Key takeaways about:
  - TLS configuration complexity (Azurite vs MinIO)  
  - Importance of clean version management  
  - Delta-RS maturity and limitations  

## Closing Thoughts
- How this exploration connects to my larger **NYC Taxi Data Engineering Lab** and data reliability theme.  
- Invitation to readers to try local Delta-RS with emulated object stores.  
- Optional LinkedIn-style closing line (encouraging dialogue or sharing experiences).

## Appendix (Optional)
- Sample directory structure
- Delta log JSON snippet
- Reference commands for MinIO and Azurite launches

---

## üïì Delta Lake Time Travel ‚Äî Direct from MinIO over HTTPS

**Fully encrypted, direct Delta Lake Time Travel operation**

---

### üîê Environment Setup

Verified MinIO‚Äôs HTTPS trust chain using the mkcert-generated CA:

```powershell
$env:CAROOT = (mkcert -CAROOT)
$env:AWS_ENDPOINT_URL      = "https://127.0.0.1:9010"
$env:AWS_CA_BUNDLE         = "$env:CAROOT\rootCA.pem"
$env:AWS_S3_FORCE_PATH_STYLE = "1"
$env:AWS_REGION            = "us-east-1"
$env:AWS_ACCESS_KEY_ID     = "minioadmin"
$env:AWS_SECRET_ACCESS_KEY = "minioadmin"
```

---

## üíº The Business Ask ‚Äî Presenting Revenue from a Specific Period

> ‚ÄúI have a presentation to make showing **Taxi Trip revenues from March 12 through May 15**.  
> I need to see metrics A, B, C, D, and E.‚Äù

This is the kind of real-world request that comes in from Finance, Operations, or Marketing.  
The timeline is narrow, but the expectation is absolute clarity:

| Metric | Description | Power BI Visualization |
|:--|:--|:--|
| **A** | Total Trip Revenue | KPI Card showing total fare + tips |
| **B** | Daily Revenue Trend | Line chart of daily totals |
| **C** | Revenue by Payment Type | Donut chart or stacked bar |
| **D** | Top 10 Pickup Zones by Revenue | Bar chart |
| **E** | Vendor Share | 100% Stacked Column comparing vendors |

Rather than running new ETL jobs or risking stale copies, the **Delta Time Travel** feature let me retrieve a precise historical snapshot of the dataset as it existed during that period.

---

### üß± Data Foundation

- Source: `s3://nyctaxi-pipeline/delta/yellowtrips` (Delta version 4)  
- Time window: **2025-03-12 ‚Üí 2025-05-15**  
- Output: a single Parquet export written directly to MinIO via HTTPS  
- Result: **53,524 rows**, cleanly scoped to the requested window  

This Parquet file is the one-click data source for Power BI:

---


From here, I can connect Power BI Desktop to MinIO using an S3-compatible endpoint or stage it temporarily in Databricks for shared analytics.

---

### üß≠ What Comes Next

Power BI will surface the visuals the business needs:

- **A** and **B** from quick DAX measures on `fare_amount`, `tip_amount`, and `total_amount`  
- **C** from a simple group-by on `payment_type`  
- **D** using the `PUlocationID` dimension lookup  
- **E** by counting distinct `VendorID` values

Databricks will join later in the workflow ‚Äî it‚Äôs still a ‚Äúthin‚Äù layer now, but will soon host the same Delta tables for shared compute and reproducible dashboards.

> ### üîë Key Takeaways
> A key takeaway: one Delta Time Travel query, one Parquet file, and the entire presentation dataset is reproducible, auditable, and securely sourced from MinIO over HTTPS.
> Secure HTTPS connection across all platforms
> Delta-RS Time Travel enabling historical accuracy
> Zero local dependencies ‚Äî all cloud-style operations

> üîë **Key Insight:** The same mkcert CA unified MinIO, SQL Server, and Python.

---

## üß† Pseudocode ‚Äî Delta Lake Time Travel (Databricks Community Edition)

1. **Create a small Delta table** in my user catalog (DBFS-backed) and note its path.  
2. **Make 2‚Äì3 writes** (insert, update, delete) to generate multiple versions.  
3. **Inspect history** with `DESCRIBE HISTORY` to confirm version numbers.  
4. **Query past snapshots** using `VERSION AS OF <n>` and `TIMESTAMP AS OF '<ts>'`.  
5. *(Optional)* Demonstrate the **`@` syntax** (`table@v123` or `table@yyyyMMddHHmmssSSS`) as an alternative.  
6. **Avoid running `VACUUM`** for now ‚Äî old Parquet files are required for Time Travel to function.

---

## ‚öôÔ∏è What Works in Databricks Community Edition (DBX CE)

- ‚úÖ **Delta Time Travel** by **version** and **timestamp** works natively in CE using both SQL and Spark APIs.  
- ‚úÖ **`DESCRIBE HISTORY`** lists operations, timestamps, and version numbers for any Delta table.  
- ‚úÖ **`@version` / `@timestamp` syntax** is supported as a shortcut alternative to `VERSION AS OF` and `TIMESTAMP AS OF`.  
- ‚ö†Ô∏è **Use DBFS-backed managed tables.** External object stores (S3, ADLS, MinIO, Azurite) and Unity Catalog features aren‚Äôt available in CE.  
- üí° **Everything happens locally** inside the user workspace; still perfect for demonstrating Delta‚Äôs ACID log and snapshot isolation behavior.

---

## üèóÔ∏è Medallion Architecture ‚Äî Three Domains Simplified

### ü•â Bronze ‚Äî Raw & Ingested
- **Purpose:** Capture data *exactly as received* from source systems ‚Äî the immutable foundation for all downstream processing.  
- **Format:** Often unstructured or semi-structured (CSV, JSON, logs, EHR extracts, HL7 messages, Parquet dumps).  
- **Characteristics:**  
  - No transformations, no filtering ‚Äî fidelity to source is paramount.  
  - **PII and PHI are often present in full form**, making this layer highly sensitive.  
  - Access should be **restricted, audited, and short-lived** where possible.  
  - Schema may be inferred or missing; quality checks begin *after* landing.  
- **Example:** Raw HL7 or FHIR data feeds containing patient identifiers, medication details, or encounter notes stored as Delta for traceability.

---

‚ö†Ô∏è **Compliance Caution:**  
The **Bronze layer is never safe for broad access**.  
Organizations typically enforce isolation at this stage ‚Äî dedicated storage accounts, ACLs, encryption at rest (TDE or SSE), and network-level restrictions ‚Äî ensuring only controlled ETL processes can read or transform data into the **Silver** layer where de-identification begins.

---

## üîí What ‚ÄúDe-Identification‚Äù Means (PII/PHI)

**Goal:** Reduce the chance a person can be identified from the data while preserving utility for analytics.

### Two regulatory paths (HIPAA)
- **Safe Harbor:** Remove the prescribed set of direct identifiers (the ‚Äú18 identifiers,‚Äù e.g., name, full address, phone, email, SSN, MRN, full-precision dates, etc.).
- **Expert Determination:** A qualified expert certifies that re-identification risk is very small given context and controls.

### Common techniques (choose per use case)
| Technique           | What it does                                  | Reversible? | Notes |
|---------------------|-----------------------------------------------|-------------|------|
| **Masking**         | Obscure parts (e.g., `555-***-****`)          | No          | Quick, lightweight; still sensitive if many fields remain. |
| **Tokenization**    | Replace with random tokens (`PAT_9F3A‚Ä¶`)      | Yes*        | Requires secure vault/map; analytics on joins via tokens. |
| **Hashing (+salt)** | Deterministic pseudonyms from identifiers     | Yes*        | Use strong hash + per-env salt; vulnerable without salt. |
| **Generalization**  | Reduce precision (e.g., age ‚Üí 10-year bins)   | No          | Key for k-anonymity; helps aggregate reporting. |
| **Suppression**     | Drop high-risk fields/rows entirely           | No          | Use for outliers/small groups. |
| **Date shifting**   | Shift dates consistently per subject          | Yes*        | Preserve intervals; keep shift secret and consistent. |
| **Aggregation**     | Summarize (counts, rates)                     | No          | Use for Gold KPIs to avoid small-cell disclosure. |

\*Reversible by a holder of keys/salts/mappings ‚Üí **pseudonymization**, not full anonymization.

### Where it fits in Medallion
- **Bronze ‚Üí Silver:** De-identification begins here. Identify sensitive columns, apply masking/tokenization/generalization, remove free-text PHI, and enforce schema + access controls. Keep an **auditable mapping** of transformations.
- **Silver ‚Üí Gold:** Prefer aggregated, minimal datasets. Apply small-cell suppression (e.g., do not show counts < 10) and expose only what reports need.

### Gotchas (easy to miss)
- **Free-text notes** often contain PHI‚Äîredact or drop.  
- **Dates & locations** can re-identify (e.g., rare events); generalize (month/quarter, 3-digit ZIP).  
- **Linkage attacks**: Even de-identified data can be re-identified when joined with external datasets; mitigate with governance, access limits, and aggregation.

**Bottom line:** In my pipeline, *de-identification is a controlled, documented step at the Silver layer* that turns raw PHI/PII into safer, analysis-ready data‚Äîwhile preserving lineage back to Bronze if regulated audits require it.

---

### ü•à Silver ‚Äî Cleaned & Structured
- **Purpose:** Provide **trusted, queryable data** for analytics and integration.  
- **Format:** Fully structured with enforced schema and data types.  
- **Characteristics:**  
  - Deduped, validated, normalized, and enriched.  
  - **PII and PHI protections are applied here** ‚Äî identifiers masked, tokenized, or removed during transition from Bronze.  
  - Data quality, referential integrity, and business rules established.  
  - Serves as the **first broadly accessible layer** under governance controls.  
- **Example:** Patient encounter data joined with provider and medication reference tables, exposing only de-identified or coded fields such as encounter date, diagnosis group, and billing category.

---

üîí **Governance Continuity:**  
Bronze captures everything for lineage; Silver enforces what‚Äôs *appropriate* for consumption.  
This is the **compliance bridge** where raw sensitivity becomes managed data ‚Äî ensuring every downstream Gold dataset remains HIPAA-aligned and enterprise-safe.

---

### ü•á Gold ‚Äî Aggregated & Business-Ready
- **Purpose:** Deliver **insight-ready** datasets for consumption (BI, ML, APIs).  
- **Format:** Star/Snowflake models, curated views, or feature tables.  
- **Characteristics:**  
  - Aggregated, summarized, and optimized for performance.  
  - Tailored to specific use cases (finance, marketing, operations).  
  - Supports dashboards, KPIs, and machine learning pipelines.  
- **Example:** Monthly revenue summaries, per-vendor performance metrics, or pre-joined datasets driving Power BI visuals.

üí° *In short:*  
**Bronze ‚Üí Silver ‚Üí Gold** represents the journey from **raw chaos to refined intelligence**, where structure, trust, and usability increase at every stage.

üîí **Compliance Note:**  
In healthcare and other regulated domains, *data classification and privacy enforcement* typically begin at the **Silver** layer.  
That‚Äôs where organizations enforce de-identification, apply data-access controls, and maintain audit logs before promoting data to **Gold**, ensuring all consumption layers remain compliant and safe.

---

## üß≠ Terminology ‚Äî Layer vs. Zone vs. Domain

| Term | Connotation | Common Usage |
|------|--------------|---------------|
| **Layer** | Logical stage of data refinement (Bronze ‚Üí Silver ‚Üí Gold). | ‚úÖ *Standard in Databricks and Delta Lake documentation.* |
| **Zone** | Often used in cloud storage contexts (e.g., raw zone, curated zone). | üü° *Valid but more storage-oriented than logical.* |
| **Domain** | Refers to a *business domain* (Finance, HR, Patient Care). | ‚öôÔ∏è *Common in data mesh or microservice architectures.* |
| **Space** | Informal; sometimes used internally to describe work areas. | üîπ *Avoid in formal architecture language.* |

**In short:**  
> *Use ‚Äúlayer‚Äù when referring to Bronze, Silver, and Gold.  
> Use ‚Äúdomain‚Äù when referring to business subject areas.*

---

## üï∞Ô∏è Time Travel Across the Medallion Layers

Delta‚Äôs **Time Travel** feature is layer-agnostic ‚Äî it functions anywhere Delta tables exist ‚Äî  
but its *intent and value* vary by stage of refinement:

### ü•â Bronze Layer ‚Äî **Forensics & Recovery**
- Use TT to **replay or re-ingest** raw data exactly as it arrived.  
- Valuable for **incident response** or verifying source integrity.  
- Example: Compare raw HL7 messages from version 0 to version 5 after ETL code changes.

### ü•à Silver Layer ‚Äî **Audit & Validation**
- TT becomes a **data-quality checkpoint**: confirm that cleansing, joins, and de-identification behaved as expected.  
- Enables **before/after comparisons** across schema-enforced snapshots.  
- Example: Inspect patient records prior to masking or deduplication.

### ü•á Gold Layer ‚Äî **Reproducibility & Traceability**
- TT supports **historical reporting** and reproducible analytics.  
- Analysts can regenerate a dashboard *as it appeared on a given date*.  
- Example: Query `gold.monthly_revenue VERSION AS OF 57` to match a published KPI report.

---

üí° **Summary:**  
Time Travel is the connective tissue between Medallion layers ‚Äî  
it preserves **lineage, reproducibility, and trust**, ensuring that every refinement step from Bronze ‚Üí Silver ‚Üí Gold can be explained, audited, and, if necessary, *rolled back in time.*

---

<!--
### Step 1 ‚Äî TextGoesHere
```bash
CodeHere
```

![YellowTrips table validation](/assets/images/screenshots/databricks-volumes.JPG)
{: .screenshot-lg }

---

[‚¨Ü Back to Top](#toc){:.back-to-top}
### Step 2 ‚Äî TextGoesHere
```bash
CodeGoesHere
```
![YellowTrips table validation](/assets/images/screenshots/databricks-samples.JPG)
{: .screenshot-sm }
```bash
CodeGoesHere
```

![YellowTrips table validation](/assets/images/screenshots/databricks-sampledata.JPG)
{: .screenshot-lg }

---

### Step 3 ‚Äî TextGoesHere
1. StepA
2. StepB  
3. StepC  

![YellowTrips table validation](/assets/images/screenshots/databricks-createvolume.JPG)
{: .screenshot-sm }

```bash
CodeGoesHere
```

![YellowTrips table validation](/assets/images/screenshots/databricks-volume.JPG)
{: .screenshot-sm }

---

[‚¨Ü Back to Top](#toc){:.back-to-top}
### Step 4 ‚Äî TextGoesHere
Catalog ‚ûú Volume (nyctaxitrip) ‚ûú Upload to volume

![YellowTrips table validation](/assets/images/screenshots/databricks-uploadtovolume.JPG)
{: .screenshot-lg }

Confirm files landed
```bash
CodeGoesHere
```

![YellowTrips table validation](/assets/images/screenshots/databricks-files.JPG)
{: .screenshot-lg }

---

### Step 5 ‚Äî TextGoesHere
```bash
CodeGoesHere
```

NuanceTextHere
{: .nuance}

![YellowTrips table validation](/assets/images/screenshots/databricks-tables.JPG)
{: .screenshot-lg }

---

[‚¨Ü Back to Top](#toc){:.back-to-top}
### Step 6 ‚Äî TextGoesHere
```bash
CodeGoesHere
```

![YellowTrips table validation](/assets/images/screenshots/databricks-describeextended.JPG)
{: .screenshot-sm }

---

### Step 7 ‚Äî TextGoesHere
```bash
CodeGoesHere
```

![YellowTrips table validation](/assets/images/screenshots/databricks-describeextended.JPG)
{: .screenshot-med }

---

[‚¨Ü Back to Top](#toc){:.back-to-top}
### Step 8 ‚Äî TextGoesHere  
```bash
CodeGoesHere
```

---

### Step 9 ‚Äî TextGoesHere  
```bash
CodeGoesHere
```

---

[‚¨Ü Back to Top](#toc){:.back-to-top}
### Step 10 ‚Äî TextGoesHere  
```bash
CodeGoesHere
```

---

### Step 11 ‚Äî TextGoesHere  
```bash
CodeGoesHere
```

---

[‚¨Ü Back to Top](#toc){:.back-to-top}
### Step 12 - TextGoesHere  
```bash
CodeGoesHere
```

---
-->

[‚¨Ü Back to Top](#toc){:.back-to-top}

