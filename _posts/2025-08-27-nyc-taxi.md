---
layout: single
title: "NYC Taxi Database Project"
excerpt: "The NYC Taxi Database Project demonstrates a full data-engineering pipeline built from messy real-world trip data. Starting with Parquet files, we generate PSV conversions, schema validations, and automated data dictionaries, then bulk-load into SQL Server and PostgreSQL. Downstream phases showcase orchestration with Airflow, advanced analytics with Databricks, and visualization in Power BI."
date: 2025-08-27
classes: wide
author_profile: false
description: ""
sidebar: false
toc: true
toc_label: "SECTIONS"
toc_icon: "list"
header:
  overlay_color: "#000"
  overlay_filter: "0.85"
  overlay_image: /assets/images/nyctaxi-hero.jpg
  caption: "NYC Taxi Trip Data Pipeline"
---

<a id="table-of-contents"></a>

<style>
  .muted { font:500 13px/1.2 system-ui,-apple-system,Segoe UI,Roboto,Arial; fill:#ddd; }
</style>

<div style="text-align:center; margin-bottom:1.5rem;">
  <img src="{{ '/assets/images/subway-roadmap.svg' | absolute_url }}?v=6" alt="Project Roadmap" style="max-width:100%; height:auto;">
  <p class="text-sm text-gray-400 muted">‚Ü≥ This project sits on the roadmap (blue baseline) as our real-world pipeline case study.</p>
</div>

## üöï Overview

The NYC Taxi dataset is a well-known public dataset that records millions of yellow and green cab trips across New York City.  
I chose it for this project because it‚Äôs **large, messy, and realistic** ‚Äî a perfect sandbox to demonstrate performance, automation, and modern data-engineering techniques.

## üéØ Goals

- Convert **Parquet ‚Üí PSV (pipe-separated)** for transparent, row-based inspection.  
- Validate schemas and generate **data dictionaries** automatically.  
- Bulk insert into **SQL Server** and **PostgreSQL**.  
- Build a pipeline that is **repeatable, automated, and documented**.  
- Downstream: showcase integration with **Airflow**, **Databricks**, and **Power BI**.

## üõ†Ô∏è Key Steps

1. **Validation scripts** (Python + DuckDB + Pandas) ensure schema alignment and catch errors.  
2. **Conversion pipeline** produces clean PSV files ready for database ingest.  
3. **Bulk insert scripts** for both SQL Server and PostgreSQL.  
4. **Repo hygiene**: `.gitignore`, venv isolation, and structured docs (`setup_steps.md`, `run_steps.md`).  

```bash
# Example: PSV Bulk Insert (SQL Server)
BULK INSERT nyctaxi.dbo.yellow_trips
FROM 'd:\appdev\nyctaxi\data_out\yellow_tripdata_2024-01.psv'
WITH (
  FIELDTERMINATOR = '|',
  ROWTERMINATOR   = '0x0a',
  TABLOCK
);
```
